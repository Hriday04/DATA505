---
title: "Wines of PNW"
subtitle: "Applied Machine Learning"
author: "Hriday Raj"
  
jupyter: python3

---
**Abstract:**

This is a technical blog post of **both** an HTML file *and* [.qmd file](src/wine_of_pnw_py.qmd) hosted on GitHub pages.

# Setup

**Set Up Python:**
```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
```

**Set Up R Compatability:**
```{python}
import pyreadr
import statsmodels.api as sm
import statsmodels.formula.api as smf
```

**Step Up Dataframe:**
```{python}
#url = 'https://cd-public.github.io/D505/dat/'
#rds = 'wine.rds'
#pyreadr.download_file(url + rds, rds)
path="/Users/hridayraj/Desktop/sp25/ml/wine.rds"
wine = pyreadr.read_r(path)[None]      
wine = wine[wine['province'].isin(['Oregon', 'California', 'New York'])]
wine['cherry'] = wine['description'].apply(lambda x: 'herry' in x)
wine['lprice'] = wine['price'].apply(np.log)
wine = wine[['lprice', 'points', 'cherry', 'province']]
```

**Explanataion:**

This code first pulls the data from a file called wine.rds then filters it by the column province based on or conditions for values.

The code then creates a column 'cherry' as a boolean for if the descripriton of the observation contains the word cherry,. 

It then creates a column 'lprice' which is the log of the price.

Finally it subsets the data into only having 4 features.

# Multiple Regression

## Linear Models

First run a linear regression model with log of price as the dependent variable and 'points' and 'cherry' as features (variables).

```{python}
m1 = smf.ols('lprice ~ points + cherry', data=wine).fit()
print("\nm1_summary\n",m1.summary())
rmse = np.sqrt(m1.mse_resid)
print(f'\nm1 RMSE: {rmse}')
```

**Explanation:**

- `sm.OLS` creates and fits a linear regression model.
- `m1.summary()` provides the regression results.
- The RMSE is 0.4688029155134235 which means on average, the model's predictions deviate from the actual values by .4688029155134235 units
## Interaction Models

Add an interaction between 'points' and 'cherry'.

```{python}
m2 = smf.ols('lprice ~ points * cherry', data=wine).fit()
print("\nm2_summary\n",m2.summary())
m2_rmse = np.sqrt(m2.mse_resid)
print(f'\nm2 RMSE: {m2_rmse}\n')
```

**Explanation:**

- Multiplies the `points` and `cherry` variables to create an interaction term.
- Repeats the same steps as before to fit the model and calculate RMSE.

### The Interaction Variable

The coefficient of the interaction term indicates the combined effect of `points` and `cherry` on the log of price. A positive coefficient suggests that the interaction increases `lprice`, while a negative value suggests the opposite.

## Applications

Determine which province (Oregon, California, or New York) does the 'cherry' feature in the data affect price most.

```{python}
ore = wine[wine["province"] == "Oregon"]
cal = wine[wine["province"] == "California"]
nyc = wine[wine["province"] == "New York"]

x_o, x_c, x_n = ore["cherry"], cal["cherry"], nyc["cherry"]
y_o, y_c, y_n = ore["lprice"], cal["lprice"], nyc["lprice"]

more = sm.OLS(y_o, x_o).fit()
mcal = sm.OLS(y_c, x_c).fit()
mnyc = sm.OLS(y_n, x_n).fit()

print("\noregon_summary\n",more.summary())
print("\ncalifornia_summary\n",mcal.summary())
print("\nnyc_summary\n",mnyc.summary())
```

**Explanation:**

- The data is split by `province` into three subsets.
- Separate regression models are run for each subset to determine the effect of the `cherry` feature on `lprice`.
- Compare the coefficients to identify which province has the highest impact.

# Scenarios

## Ignorance is no Excuse

Dropping sensitive features like age, income, or gender does not guarantee an ethical model. Other correlated variables may act as proxies, perpetuating bias indirectly.

Imagine a model to distinguish New York wines from those in California and Oregon. After a few days of work, you take some measurements and note: "I've achieved 91% accuracy on my model!" 

Should you be impressed? Why or why not?

I should not be very impressed since the california and Oregon wines far outnumber the other New York observations accounting for roughly 25/27 of the data so a model that only guesses the majority class would be 91% accurate.


## On Ethics

Why is understanding this vignette important to use machine learning in an ethical manner?

Our training of models matters only as much as the data Quality and we have to account for the data when training models.


## Ignorance is no excuse
Imagine you are working on a model to predict the likelihood that an individual loses their job as the result of the changing federal policy under new presidential administrations. You have a very large dataset with many hundreds of features, but you are worried that including indicators like age, income or gender might pose some ethical problems. When you discuss these concerns with your boss, she tells you to simply drop those features from the model. Does this solve the ethical issue? Why or why not?

This does not solve the ethical issue because without specfic finetuning of the data set it might end up categorizing those groups through indirect metrics to group those category.
